{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignent 3\n",
    "### Team 26"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d8fc6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e689a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../myDataFile.csv\", low_memory=False)\n",
    "df.fillna(0, inplace=True)\n",
    "df.replace('t', 1, inplace=True)\n",
    "df.to_csv(\"../data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4f9f7b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      Instant_food_products  UHT_milk  abrasive_cleaner  artif__sweetener  \\\n0                         0         0                 0                 0   \n1                         0         0                 0                 0   \n2                         0         0                 0                 0   \n3                         0         0                 0                 0   \n4                         0         0                 0                 0   \n...                     ...       ...               ...               ...   \n9830                      0         0                 0                 0   \n9831                      0         0                 0                 0   \n9832                      0         0                 0                 0   \n9833                      0         0                 0                 0   \n9834                      0         0                 0                 0   \n\n      baby_cosmetics  baby_food  bags  baking_powder  bathroom_cleaner  beef  \\\n0                  0          0     0              0                 0     0   \n1                  0          0     0              0                 0     0   \n2                  0          0     0              0                 0     0   \n3                  0          0     0              0                 0     0   \n4                  0          0     0              0                 0     0   \n...              ...        ...   ...            ...               ...   ...   \n9830               0          0     0              0                 0     1   \n9831               0          0     0              0                 0     0   \n9832               0          0     0              0                 0     0   \n9833               0          0     0              0                 0     0   \n9834               0          0     0              0                 0     0   \n\n      ...  turkey  vinegar  waffles  whipped_sour_cream  whisky  white_bread  \\\n0     ...       0        0        0                   0       0            0   \n1     ...       0        0        0                   0       0            0   \n2     ...       0        0        0                   0       0            0   \n3     ...       0        0        0                   0       0            0   \n4     ...       0        0        0                   0       0            0   \n...   ...     ...      ...      ...                 ...     ...          ...   \n9830  ...       0        0        0                   1       0            0   \n9831  ...       0        0        0                   0       0            0   \n9832  ...       0        0        0                   0       0            0   \n9833  ...       0        0        0                   0       0            0   \n9834  ...       0        1        0                   0       0            0   \n\n      white_wine  whole_milk  yogurt  zwieback  \n0              0           0       0         0  \n1              0           0       1         0  \n2              0           1       0         0  \n3              0           0       1         0  \n4              0           1       0         0  \n...          ...         ...     ...       ...  \n9830           0           1       0         0  \n9831           0           0       0         0  \n9832           0           0       1         0  \n9833           0           0       0         0  \n9834           0           0       0         0  \n\n[9835 rows x 169 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Instant_food_products</th>\n      <th>UHT_milk</th>\n      <th>abrasive_cleaner</th>\n      <th>artif__sweetener</th>\n      <th>baby_cosmetics</th>\n      <th>baby_food</th>\n      <th>bags</th>\n      <th>baking_powder</th>\n      <th>bathroom_cleaner</th>\n      <th>beef</th>\n      <th>...</th>\n      <th>turkey</th>\n      <th>vinegar</th>\n      <th>waffles</th>\n      <th>whipped_sour_cream</th>\n      <th>whisky</th>\n      <th>white_bread</th>\n      <th>white_wine</th>\n      <th>whole_milk</th>\n      <th>yogurt</th>\n      <th>zwieback</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9830</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9831</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9832</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9833</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9834</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>9835 rows Ã— 169 columns</p>\n</div>"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing the frequent item sets\n",
    "The code below computes the frequent item sets per layer.\n",
    "\n",
    "### Apriori algorithm\n",
    "We first generate the candidates for a given layer using self-joining. For each item want to add to the candidate list however, we first check if it holds up to pruning. Only then do we add it.\n",
    "\n",
    "#### Self-joining\n",
    "This step is performed inside the function below. We take each pair of frequent item sets from the previous layer and create a potential candidate for the current layer by merging the 2 item sets. We make sure the two item sets of the previous layer only differ by one element.\n",
    "\n",
    "#### Pruning\n",
    "Before appending a potential candidate to the candidates collection, we check if all its k-1 subsets are frequent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "97b7ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequent_item_sets(df, min_support):\n",
    "\n",
    "    # Define first layer candidates in a dictionary\n",
    "    candidates = {\n",
    "        1: [{item} for item in list(df.columns)]\n",
    "    }\n",
    "\n",
    "    # Compute first layer frequent item sets.\n",
    "    layers = {\n",
    "        1: pick_candidates_for_layer(candidates[1], df, min_support)\n",
    "    }\n",
    "\n",
    "    n_columns = df.shape[1]\n",
    "    for layer in range(2, n_columns + 1):\n",
    "\n",
    "        # Step 1 - Self-join. Compute candidates for current layer\n",
    "        candidates[layer] = list()\n",
    "        for p in range(len(layers[layer - 1]) - 1):\n",
    "\n",
    "            for q in range(p + 1, len(layers[layer - 1])):  # start at p+1 to avoid duplicates.\n",
    "\n",
    "                p_set = layers[layer - 1][p]\n",
    "                q_set = layers[layer - 1][q]\n",
    "\n",
    "                # Check if the number of common elements between the two item sets equals layer - 2.\n",
    "                # If so, they differ by one element and a new candidate (union) can be computed.\n",
    "                if len(p_set.intersection(q_set)) == layer - 2:\n",
    "                    new_candidate = p_set.union(q_set)\n",
    "\n",
    "                    # Only append if not a duplicate\n",
    "                    if not new_candidate in candidates[layer]:\n",
    "\n",
    "                        # Step 2 - Pruning (based on existence of subsets in previous layer)\n",
    "                        subsets_of_new_candidate = set(itertools.combinations(new_candidate, layer - 1))\n",
    "                        if all([set(subset) in layers[layer - 1] for subset in subsets_of_new_candidate]):\n",
    "                            candidates[layer].append(new_candidate)\n",
    "\n",
    "\n",
    "        # Pick item sets for layer if they exceed the minimum support.\n",
    "        picked = pick_candidates_for_layer(candidates[layer], df, min_support)\n",
    "\n",
    "        # Break early if no more frequent item sets are found.\n",
    "        if len(picked) == 0:\n",
    "            break\n",
    "\n",
    "        layers[layer] = picked\n",
    "\n",
    "        print(\"Finished computing for layer \" + str(layer))\n",
    "\n",
    "    return layers, candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing support\n",
    "The function below returns the support metric for a given `item_set`. We first need to check if the `item_set` is valid (i.e. has at least one item) before computing. This computation is done by simply looking up values using a Pandas DataFrame query."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def compute_support_for(df, item_set):\n",
    "    # df is the (reference to) complete dataframe\n",
    "    # item_set is a set of strings, each being a column name in df to be checked.\n",
    "    # returns the support percentage for a given item_set.\n",
    "\n",
    "    if len(item_set) == 0:\n",
    "        return 0\n",
    "\n",
    "    query = \" == 1 and \".join(item_set) + \" == 1\"\n",
    "    return df.query(query).shape[0] / df.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Picking candidates\n",
    "The following function prunes the candidate itemsets after evaluating their support. If they exceed the minimum value passed as a parameter, then the itemset will go into L (the final frequent itemsets collection)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "def pick_candidates_for_layer(candidates, df, minSup):\n",
    "    return [item_set for item_set in candidates if compute_support_for(df, item_set) >= minSup]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rule generation\n",
    "The method below generates all rules for a given collection of frequent item sets. It does this by, for each layer and each frequent item set in that layer, computing the available rules for that item set.\n",
    "\n",
    "Note that we skip the first layer, as no rules can be generated using only one item in the item set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "def generate_all_rules(df, layers, min_conf, rules):\n",
    "\n",
    "    # For each layer of the item sets\n",
    "    for k, item_sets in layers.items():\n",
    "\n",
    "        # No rules can be formed out of item sets with length 1.\n",
    "        if k == 1:\n",
    "            continue\n",
    "\n",
    "        # Per item set in this layer...\n",
    "        for item_set in item_sets:\n",
    "\n",
    "            # ... compute the rules\n",
    "            compute_rules_for(df, item_set, set([]), min_conf, rules)\n",
    "\n",
    "        print(\"Computed rules for item sets of length \" + str(k) + \":\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing rules from an item set\n",
    "Given an item set in the layer, we use a recursive method to generate all possible rules from that item set.\n",
    "\n",
    "In order to do this we use the non-monotonicity property. This means we start out with a large antecedent, and as we go deeper into the recursion we move element from the antecedent set to the consequent set. We stop the branch of the recursion if the rule does not mean the minimum confidence. Therefor we use the non-monotonicity property to stop early with checking possble rules. Moreover, we skip possible duplicates by checking if the rule is already in the dictionary.\n",
    "\n",
    "For each (unique) possible rule, we then compute the confidence level and store the rule if it meets or exceeds the threshold, as we've interpreted the minimum confidence this way.\n",
    "\n",
    "Note that, due to Python's pass-by-reference policy, we need to copy existing item sets, in order to not lose original item set we branch from."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "def compute_rules_for(df, antecedents, consequents, min_conf, rules):\n",
    "\n",
    "    # Base case, no more consequents for valid rule.\n",
    "    if len(antecedents) == 0:\n",
    "        return rules\n",
    "\n",
    "    # For each consequent\n",
    "    for antecedent in antecedents:\n",
    "\n",
    "        next_consequents = consequents.copy()\n",
    "        next_consequents.add(antecedent)  # add single antecedent to consequent set.\n",
    "\n",
    "        # Create copy (because python) of antecedents and remove one antecedent.\n",
    "        next_antecedents = antecedents.copy()\n",
    "        next_antecedents.remove(antecedent)\n",
    "\n",
    "        # Skip duplicates\n",
    "        if frozenset(next_antecedents) in rules and frozenset(next_consequents) in rules[frozenset(next_antecedents)]:\n",
    "            continue\n",
    "\n",
    "        # Compute confidence for possible new rule.\n",
    "        conf = compute_confidence(df, next_antecedents, next_consequents)\n",
    "\n",
    "        # If rule passes min confidence, add to rules\n",
    "        if conf >= min_conf:\n",
    "\n",
    "            # Add to the dict\n",
    "            if frozenset(next_antecedents) in rules:\n",
    "                rules[frozenset(next_antecedents)][frozenset(next_consequents)] = conf\n",
    "            else:\n",
    "                rules[frozenset(next_antecedents)] = {\n",
    "                    frozenset(next_consequents) : conf\n",
    "                }\n",
    "\n",
    "            if len(next_antecedents) > 1:\n",
    "                # Only then, go into recursion on these item sets.\n",
    "                compute_rules_for(df, next_antecedents, next_consequents, min_conf, rules)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing confidence\n",
    "Lastly, computing the confidence level is done simply by using the previously mentioned `compute_support_for()` method in the formula for computing confidence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# takes 3 input parameters, the reference to the dataframe and 2 item sets\n",
    "def compute_confidence(df, antecedent, consequent):\n",
    "    return compute_support_for(df, set.union(antecedent, consequent)) / compute_support_for(df, antecedent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "5b9ee4b3",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b707cbee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computing for layer 2\n",
      "Finished computing for layer 3\n",
      "Finished computing for layer 4\n"
     ]
    }
   ],
   "source": [
    "# First, compute item sets\n",
    "L, C = compute_frequent_item_sets(df, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed rules for item sets of length 2:\n",
      "Computed rules for item sets of length 3:\n",
      "Computed rules for item sets of length 4:\n"
     ]
    }
   ],
   "source": [
    "rules = {}\n",
    "\n",
    "# Then generate the rules\n",
    "generate_all_rules(df, L, 0.6, rules)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths for L:\n",
      "1: 120\n",
      "2: 605\n",
      "3: 264\n",
      "4: 12\n"
     ]
    }
   ],
   "source": [
    "# Show results for computing the item sets\n",
    "print(\"Lengths for L:\")\n",
    "for key, value in L.items():\n",
    "    print(str(key) + \": \" + str(len(value)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{bottled_water, butter} -> {whole_milk} (confidence = 0.6022727272727273)\n",
      "{domestic_eggs, butter} -> {whole_milk} (confidence = 0.6210526315789474)\n",
      "{butter, root_vegetables} -> {whole_milk} (confidence = 0.6377952755905512)\n",
      "{tropical_fruit, butter} -> {whole_milk} (confidence = 0.6224489795918368)\n",
      "{whipped_sour_cream, butter} -> {whole_milk} (confidence = 0.66)\n",
      "{butter, yogurt} -> {whole_milk} (confidence = 0.6388888888888888)\n",
      "{tropical_fruit, curd} -> {whole_milk} (confidence = 0.6336633663366337)\n",
      "{domestic_eggs, margarine} -> {whole_milk} (confidence = 0.6219512195121952)\n",
      "{pip_fruit, domestic_eggs} -> {whole_milk} (confidence = 0.6235294117647059)\n",
      "{domestic_eggs, tropical_fruit} -> {whole_milk} (confidence = 0.6071428571428571)\n",
      "{onions, root_vegetables} -> {other_vegetables} (confidence = 0.6021505376344086)\n",
      "{pip_fruit, whipped_sour_cream} -> {other_vegetables} (confidence = 0.6043956043956045)\n",
      "{pip_fruit, whipped_sour_cream} -> {whole_milk} (confidence = 0.6483516483516485)\n",
      "{whole_milk, citrus_fruit, root_vegetables} -> {other_vegetables} (confidence = 0.6333333333333333)\n",
      "{other_vegetables, fruit_vegetable_juice, yogurt} -> {whole_milk} (confidence = 0.6172839506172838)\n",
      "{pip_fruit, other_vegetables, root_vegetables} -> {whole_milk} (confidence = 0.675)\n",
      "{pip_fruit, whole_milk, root_vegetables} -> {other_vegetables} (confidence = 0.6136363636363636)\n",
      "{pip_fruit, other_vegetables, yogurt} -> {whole_milk} (confidence = 0.625)\n",
      "{whipped_sour_cream, other_vegetables, root_vegetables} -> {whole_milk} (confidence = 0.6071428571428571)\n",
      "{other_vegetables, yogurt, root_vegetables} -> {whole_milk} (confidence = 0.6062992125984252)\n",
      "{other_vegetables, yogurt, tropical_fruit} -> {whole_milk} (confidence = 0.6198347107438016)\n",
      "{tropical_fruit, yogurt, root_vegetables} -> {whole_milk} (confidence = 0.7000000000000001)\n",
      "Rule count: 22\n"
     ]
    }
   ],
   "source": [
    "# Method to prettify frozenset strings.\n",
    "def pretty(item_set):\n",
    "    return \"{\" + \", \".join(item_set) + \"}\"\n",
    "\n",
    "# Show results for the rules\n",
    "count = 0\n",
    "for antecedent in rules.keys():\n",
    "    # print(\"Ant: \" + str(antecedent) + \", con: \" + str(rules[antecedent]))\n",
    "    count += len(rules[antecedent].keys())\n",
    "\n",
    "    for consequent in rules[antecedent].keys():\n",
    "        print(pretty(antecedent) + \" -> \" + pretty(consequent) + \" (confidence = \" + str(rules[antecedent][consequent]) + \")\")\n",
    "\n",
    "print(\"Rule count: \" + str(count))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}